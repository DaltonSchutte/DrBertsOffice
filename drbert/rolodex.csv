model_name,name,weight_path,citation,backend,transformer_architecture,pretrain_dataset,additional_train_notes,misc
pubmed-abs,PubMed BERT (Abstracts),microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract,"Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon: Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing, 2020; arXiv:2007.15779.",PyTorch,BERT Base,14 million PubMed abstracts,Did not use pre-trained BERT models as base,
pubmed-abs-ft,PubMed BERT (Abstracts + Full Text),microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext,"Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon: Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing, 2020;ÊarXiv:2007.15779.",PyTorch,BERT Base,14 million PubMed abstracts and full-text articles,Did not use pre-trained BERT models as base,
bio-uncased,BioBERT Uncased,dmis-lab/biobert-base-cased-v1.1,"Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang: BioBERT: a pre-trained biomedical language representation model for biomedical text mining, 2019; [http://arxiv.org/abs/1901.08746 arXiv:1901.08746]. DOI: [https://dx.doi.org/10.1093/bioinformatics/btz682 10.1093/bioinformatics/btz682].",PyTorch,BERT Base,1 million PubMed abstracts,Weights initialized from BERT then further trained on notes,"Paths for loading via huggingface are swapped, 'biobert-base-cased-v1.1' has do_lower_case=True"
bio-cased,BioBERT Cased,dmis-lab/biobert-v1.1,"Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang: BioBERT: a pre-trained biomedical language representation model for biomedical text mining, 2019; [http://arxiv.org/abs/1901.08746 arXiv:1901.08746]. DOI: [https://dx.doi.org/10.1093/bioinformatics/btz682 10.1093/bioinformatics/btz682].",PyTorch,BERT Base,1 million PubMed abstracts,Weights initialized from BERT then further trained on notes,"Paths for loading via huggingface are swapped, 'biobert-v1.1' has do_lower_case=False"
bio-clinical,Clinical BioBERT,emilyalsentzer/Bio_ClinicalBERT,"Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, Matthew B. A. McDermott: Publicly Available Clinical BERT Embeddings, 2019; [http://arxiv.org/abs/1904.03323 arXiv:1904.03323].",PyTorch,BERT Base,2 million MIMIC-III notes,Weights initialized from BioBERT then further trained on notes,
bio-discharge,Discharge BioBERT,emilyalsentzer/Bio_Discharge_Summary_BERT,"Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, Matthew B. A. McDermott: Publicly Available Clinical BERT Embeddings, 2019; [http://arxiv.org/abs/1904.03323 arXiv:1904.03323].",PyTorch,BERT Base,MIMIC-III discharge summaries only,Weights initialized from BioBERT then further trained on notes,
clinical,Clinical BERT,NotImplemented(Need to work on TF conversion script),"Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, Matthew B. A. McDermott: Publicly Available Clinical BERT Embeddings, 2019; [http://arxiv.org/abs/1904.03323 arXiv:1904.03323].",TensorFlow,BERT Base,2 million MIMIC-III notes,Weights initialized from BERT then further trained on notes,
discharge,Discharge BERT,NotImplemented(Need to work on TF conversion script),"Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, Matthew B. A. McDermott: Publicly Available Clinical BERT Embeddings, 2019; [http://arxiv.org/abs/1904.03323 arXiv:1904.03323].",TensorFlow,BERT Base,MIMIC-III discharge summaries only,Weights initialized from BERT then further trained on notes,
sci-uncased,SciBERT Uncased SciVocab,allenai/scibert_scivocab_uncased,"Iz Beltagy, Kyle Lo, Arman Cohan: SciBERT: A Pretrained Language Model for Scientific Text, 2019, EMNLP 2019; [http://arxiv.org/abs/1903.10676 arXiv:1903.10676].",PyTorch,BERT Base,1.14M Semantic Scholar full-text papers,"Did not use pre-trained BERT models as base. Corpus 18% CompSci, 82% BioMed papers",
sci-cased,SciBERT Cased SciVocab,allenai/scibert_scivocab_cased,"Iz Beltagy, Kyle Lo, Arman Cohan: SciBERT: A Pretrained Language Model for Scientific Text, 2019, EMNLP 2019; [http://arxiv.org/abs/1903.10676 arXiv:1903.10676].",PyTorch,BERT Base,1.14M Semantic Scholar full-text papers,"Did not use pre-trained BERT models as base. Corpus 18% CompSci, 82% BioMed papers",
blue-no-mimic,Blue BERT (PubMed),bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12,"Yifan Peng, Shankai Yan, Zhiyong Lu: Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets, 2019; [http://arxiv.org/abs/1906.05474 arXiv:1906.05474].",PyTorch,BERT Base,>4B words from PubMed abstracts,Weights initialized from BERT then further trained on notes,
blue-mimic,Blue BERT (PubMed + Mimic),bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12,"Yifan Peng, Shankai Yan, Zhiyong Lu: Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets, 2019; [http://arxiv.org/abs/1906.05474 arXiv:1906.05474].",PyTorch,BERT Base,>4B words from PubMed abstracts and >500M words from MIMIC-III notes,Weights initialized from BERT then further trained on notes,
base-uncased,Bert Base Uncased,bert-base-uncased,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018;ÊarXiv:1810.04805",PyTorch,BERT Base,BooksCorpus and English Wikipedia,Masked LM and NSP,
base-cased,Bert Base Cased,bert-base-cased,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018;ÊarXiv:1810.04805",PyTorch,BERT Base,BooksCorpus and English Wikipedia,Masked LM and NSP,